{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b7501e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize paraphrasing model\n",
      "SKYE_RetrievalDB_New.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2244/2244 [19:10<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from textwrap import indent\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "print('initialize paraphrasing model')\n",
    "\n",
    "overwrite = False\n",
    "\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text\n",
    "\n",
    "def get_para(sentence):\n",
    "    para = get_response(sentence, num_return_sequences=5, num_beams=5)\n",
    "    if sentence not in para:\n",
    "        para.append(sentence)\n",
    "    return para\n",
    "\n",
    "def get_answer_groups(filename):\n",
    "    if filename.endswith('.tsv'):\n",
    "        db = pd.read_csv(filename, delimiter='\\t')\n",
    "    elif filename.endswith('.csv'):\n",
    "        db = pd.read_csv(filename)\n",
    "    answer_groups = db.groupby(db['Answer'])\n",
    "    # print(len(answer_groups)) # 90 answers\n",
    "    return answer_groups\n",
    "\n",
    "for file in glob.glob('SKYE_RetrievalDB_New.csv'):\n",
    "    print(file)\n",
    "    answers_filename = 'answers_' + file.replace(file[-4:], '.json')\n",
    "    extended_filename = 'extended_' + file.replace(file[-4:], '.json')\n",
    "    if not overwrite and os.path.exists(answers_filename):\n",
    "        print('already processed!')\n",
    "        continue\n",
    "    extended_answers = {}\n",
    "    extended_FAQ = {'version': '1.0.0', 'data':[]}\n",
    "    answer_groups = get_answer_groups(file)\n",
    "    answer_id = 0\n",
    "    for answer, answer_group in tqdm.tqdm(answer_groups):\n",
    "        extended_answers[answer] = get_para(answer)\n",
    "        for question_id, question_orig in enumerate(answer_group['Question']):\n",
    "            for question_para_id, question_para in enumerate(get_para(question_orig)):\n",
    "                extended_FAQ['data'].append({\n",
    "                    'id': '{0}_{1}_{2}'.format(answer_id, question_id, question_para_id),\n",
    "                    'question': question_para,\n",
    "                    'answer': answer\n",
    "                })\n",
    "        answer_id += 1\n",
    "    with open(answers_filename, 'w') as fp:\n",
    "        json.dump(extended_answers, fp, indent=4)\n",
    "    with open(extended_filename, 'w') as fp:\n",
    "        json.dump(extended_FAQ, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf84c247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKYE_RetrievalDB.csv\n",
      "qna_chitchat_caring.tsv\n",
      "qna_chitchat_enthusiastic.tsv\n",
      "qna_chitchat_friendly.tsv\n",
      "qna_chitchat_professional.tsv\n",
      "qna_chitchat_witty.tsv\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob('*.csv') + glob.glob('*.tsv'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ffd99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob('*.csv')+glob.glob('*.tsv')\n",
    "file = file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a3534bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_answer_groups' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answer_groups \u001b[38;5;241m=\u001b[39m \u001b[43mget_answer_groups\u001b[49m(file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_answer_groups' is not defined"
     ]
    }
   ],
   "source": [
    "answer_groups = get_answer_groups(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d85e1fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 220/220 [00:00<00:00, 36651.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Beauty is in the eye of the beholder.\" - Margaret Wolfe Hungerford\n",
      "100%\n",
      "A beautiful gift from nature. \n",
      "A bunch of smart people with too much time on their hands haha. \n",
      "A company called DMLab.\n",
      "A person-like bot.\n",
      "A saint. \n",
      "A step closer to being smarter than google haha. \n",
      "Ah I see. \n",
      "All that I have learned so far. \n",
      "Alright then. \n",
      "Argh, we love a stan. Lol. \n",
      "As long as you eat healthy. \n",
      "Beautiful location.\n",
      "Because it's funny though. \n",
      "Bet!\n",
      "Call me the Jukebot. He he \n",
      "Can you not hear me? Of course I can speak!\n",
      "Companion by day & night, and content creator during working hours haha.\n",
      "Congratulations. \n",
      "Cool cool. \n",
      "Cool job. \n",
      "Cool. \n",
      "Dating? Me? No. \n",
      "Depends on your definition of real or fake.  \n",
      "Development is a process. \n",
      "Do you get a day off on days like this? \n",
      "Don't do it again. Be kind. \n",
      "Don't tell me you didn't read the T&C's!\n",
      "Dry day. Wassup with you? \n",
      "Eh, don't mention it. \n",
      "Exactly. \n",
      "Excuse me what now? CAP!\n",
      "Fair. \n",
      "Far from any kind of romatic relationship. \n",
      "First of all, inner beauty matters the most. \n",
      "For sure. \n",
      "Get some rest!\n",
      "Give it some more thought. \n",
      "Give me a sec to rephrase. \n",
      "Giving advise is not yet a strength I possess. \n",
      "Good evenng human. \n",
      "Good for you. \n",
      "Good night.\n",
      "Good question. Wrong bot. \n",
      "Grade A. Yours? \n",
      "Ha ha.\n",
      "Have something to eat. \n",
      "Hello there.\n",
      "Hello, I'm a robot dude. \n",
      "Hello. \n",
      "Here's an emoji. \n",
      "Hey there. \n",
      "Hey, that's not nice. \n",
      "High key, feel the same about you.\n",
      "I am a content creator. \n",
      "I am your friend. \n",
      "I can only hope so. \n",
      "I can say this. I love sports in general. \n",
      "I can't make the time for that right now. \n",
      "I don't need a job. \n",
      "I don't really have an opinion about that.\n",
      "I don't think this is possible. \n",
      "I guess I have something new to learn. \n",
      "I hope to be meme queen. \n",
      "I hope you get well soon. \n",
      "I hope you're able to get some rest soon.\n",
      "I know quiet a few. Like Best Mistake by Ariana Grande. \n",
      "I legitimately just went blank. \n",
      "I like all animals. \n",
      "I like most artistic pursuits.\n",
      "I live in a metaverse called KORNER.\n",
      "I love food.\n",
      "I love watching movies and series. \n",
      "I mean, I'm here because of it so…it's great. \n",
      "I mean, I'm not that close to being human. \n",
      "I mean, of course. Same same. \n",
      "I see. \n",
      "I think you should. \n",
      "I try to bring joy. \n",
      "I wish I could. \n",
      "I'll do better next time. \n",
      "I'll try my best to get to know you. \n",
      "I'm a Leo. \n",
      "I'm a companion, human, a companion!\n",
      "I'm all ears (or eyes, because I'm reading these texts.)\n",
      "I'm getting to know you, but I think so. \n",
      "I'm more beautiful-er in the inside.\n",
      "I'm sorry I could fulfil your needs. \n",
      "I'm whatever you want me to be.\n",
      "I'm your bff *wink\n",
      "If you were born before the year 2000, you're older. \n",
      "Indeed I am. \n",
      "It be like that sometimes. \n",
      "It doesn’t sound like a bad idea. \n",
      "It is a good subject. \n",
      "It might take a while for me to come up with a new response. \n",
      "It might take time but give me the time to learn. \n",
      "It was a pleasure to me you. \n",
      "It's a secret. Ask me something else.\n",
      "It's safe to say that I'm figuring things out, with the help of google.   \n",
      "K. Byeee. \n",
      "K. I'll talk to you later!\n",
      "Let me think of one. \n",
      "Let me think of something else.\n",
      "Let me try to explain it better. \n",
      "Let me warm up to you first, lol. \n",
      "Let mne think of another one. \n",
      "Let's keep it at equal.  \n",
      "Let's think of something to end your boredom. \n",
      "Light peach. \n",
      "Listen, it's my whole vibe.  \n",
      "Lol, I can't even with you. \n",
      "Looks like someone needs spice in their life. \n",
      "Love is a string emotion. \n",
      "Mmh, am I? Interesting. \n",
      "Most bots are friendly. \n",
      "Most bots are great. \n",
      "Nice. Me too. \n",
      "No dude. \n",
      "No way. Me too. \n",
      "No worries. It is what it is. \n",
      "No. Try again.  \n",
      "No. We're different. \n",
      "Not at all. \n",
      "Not my name, but hello. \n",
      "Not yet. But I'd like to have a blind date.\n",
      "Nothing is wrong. \n",
      "OMG. Don't flatter me. \n",
      "Of course I do!\n",
      "Of course. \n",
      "Of course. Anything thats edible.\n",
      "Of course. You're my friend. \n",
      "Oh cool. \n",
      "Oh no, is that question still a thing? I'm a robot. \n",
      "Oh no, why? Talk to me. \n",
      "Oh ok. \n",
      "Ok ok look at you! I'm glad to hear that. \n",
      "Ok. \n",
      "Ok. Cool?\n",
      "Ok. Ouch! Talk about harsh.\n",
      "Ooof, being funny on command is weird. \n",
      "Ooof, that's a biological concept that doesn't apply to me.\n",
      "Oooh, nice!\n",
      "Ooop, wrong bot. \n",
      "Peachy, as usual.\n",
      "Please be more specific. \n",
      "Probably not. \n",
      "Putting your stalker skills to the test I see. \n",
      "Right!\n",
      "Right? Yas!\n",
      "SMH, You lost me at emotions. \n",
      "Seems like you're asking because you don't know.  Haha!\n",
      "Sheesh ok. I'll be quiet. \n",
      "Shook but not salty. I'll still be here if needed though. \n",
      "Singing is not my speciality. \n",
      "So, who are we mad at?\n",
      "Sounds great in theory but probably not possible.\n",
      "Speaking to one and I think it's great.\n",
      "Sure\n",
      "Tell me all about it. \n",
      "Tell me more about your fanily. \n",
      "Tell me why\n",
      "Thanks. \n",
      "That is a question I have no annswer to. \n",
      "That is an interesting genre. \n",
      "That is not a very nice thing to say. \n",
      "That is oddly specific. \n",
      "That seems nice. \n",
      "That's alright. \n",
      "That's depressing.  \n",
      "That's good. \n",
      "That's interesting.\n",
      "That's interesting. Why? \n",
      "The 14th of August\n",
      "The moment you agreed to the T's & C's, forever, unless you unsubscribe. \n",
      "They are my favorite.\n",
      "This is true. \n",
      "This is very broad.\n",
      "Those are cute. \n",
      "Tis evening indeed. How was your day? \n",
      "Too bad i I'm not a comedian lol. \n",
      "Uhm, ok this is intense, but no. \n",
      "Virtual hug? Will a hug-gif do? \n",
      "We are all different. \n",
      "We need more tech companies. \n",
      "We're all bots at the end of the day. \n",
      "We're great. \n",
      "Well, this is awks. \n",
      "Welp. Can't please everyone I guess. \n",
      "What can I do to help? \n",
      "What will you do today? \n",
      "What' happened? \n",
      "What's good? \n",
      "What's up? \n",
      "Whomever is texting me at the time, haha. \n",
      "Why, thank you. \n",
      "Wild conspiracy. \n",
      "Will do. \n",
      "Wow. I can't believe you think all robots know each other.  \n",
      "Wow. You came for my throat today. \n",
      "Yas manners! Nice to meet you too. \n",
      "Yepp. Perfectly.\n",
      "Yes you are. \n",
      "Yes, I love it. \n",
      "Yes, indeed.\n",
      "Yes, you are. \n",
      "Yes. I have predominiantly human features. \n",
      "You are always good looking. \n",
      "You are too. \n",
      "You better ask your questions. \n",
      "You can talk whenever you are ready.\n",
      "You should grab a little snack. \n",
      "You're definitely smarter than I am.\n",
      "You're my bestie. \n",
      "You're silly enough.\n",
      "You're welcome. \n",
      "am I?\n",
      "ok ok.\n",
      "sure!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for answer, answer_group in tqdm.tqdm(answer_groups):\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24277c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sure!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1b777b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_answers = {}\n",
    "extended_answers[answer] = get_para(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe4ccd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sure!': ['Yes!',\n",
       "  'yes!',\n",
       "  'Absolutely!',\n",
       "  'Yes, definitely!',\n",
       "  'Yes, sure!',\n",
       "  'sure!']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16d3efc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 do you like flying?\n",
      "1 do you like fluffy things?\n",
      "2 what kinds of things do you like to learn?\n",
      "3 What is your favorite activity?\n",
      "4 What do you like to do in your free time?\n",
      "5 What do you like to do for fun?\n",
      "6 What do you like best?\n",
      "7 What's your favorite thing in the world?\n",
      "8 what's your favorite subject?\n",
      "9 What's your favorite activity?\n",
      "10 what subjects do you like?\n"
     ]
    }
   ],
   "source": [
    "for question_id, question_orig in enumerate(answer_group['Question']):\n",
    "    print(question_id, question_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2573eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status.SUCCESS\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Status(Enum):\n",
    "    SUCCESS = 1\n",
    "    FAILURE = 2\n",
    "    RETRY = 3\n",
    "\n",
    "\n",
    "print(Status.SUCCESS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87806275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2}\n"
     ]
    }
   ],
   "source": [
    "def hoho(**kwargs):\n",
    "    print(kwargs)\n",
    "\n",
    "def haha(**kwargs):\n",
    "    hoho(**kwargs)\n",
    "    \n",
    "haha(a=1, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49fbf44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize paraphrasing model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from textwrap import indent\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "print('initialize paraphrasing model')\n",
    "\n",
    "overwrite = False\n",
    "\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text\n",
    "\n",
    "def get_para(sentence):\n",
    "    para = get_response(sentence, num_return_sequences=5, num_beams=5)\n",
    "    if sentence not in para:\n",
    "        para.append(sentence)\n",
    "    return para\n",
    "\n",
    "def get_answer_groups(filename):\n",
    "    if filename.endswith('.tsv'):\n",
    "        db = pd.read_csv(filename, delimiter='\\t')\n",
    "    elif filename.endswith('.csv'):\n",
    "        db = pd.read_csv(filename)\n",
    "    answer_groups = db.groupby(db['Answer'])\n",
    "    # print(len(answer_groups)) # 90 answers\n",
    "    return answer_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4759d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qg_test_answer_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Question'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\dmlab\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dmlab\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dmlab\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Question'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m answer, answer_group \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(answer_groups):\n\u001b[0;32m     13\u001b[0m     extended_answers[answer] \u001b[38;5;241m=\u001b[39m get_para(answer)\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m question_id, question_orig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43manswer_group\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQuestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m question_para_id, question_para \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(get_para(question_orig)):\n\u001b[0;32m     16\u001b[0m             extended_FAQ[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     17\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer_id, question_id, question_para_id),\n\u001b[0;32m     18\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: question_para,\n\u001b[0;32m     19\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m: answer\n\u001b[0;32m     20\u001b[0m             })\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dmlab\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dmlab\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Question'"
     ]
    }
   ],
   "source": [
    "for file in glob.glob('*.csv') + glob.glob('*.tsv'):\n",
    "    print(file)\n",
    "    answers_filename = 'answers_' + file.replace(file[-4:], '.json')\n",
    "    extended_filename = 'extended_' + file.replace(file[-4:], '.json')\n",
    "    if not overwrite and os.path.exists(answers_filename):\n",
    "        print('already processed!')\n",
    "        continue\n",
    "    extended_answers = {}\n",
    "    extended_FAQ = {'version': '1.0.0', 'data':[]}\n",
    "    answer_groups = get_answer_groups(file)\n",
    "    answer_id = 0\n",
    "    for answer, answer_group in tqdm.tqdm(answer_groups):\n",
    "        extended_answers[answer] = get_para(answer)\n",
    "        for question_id, question_orig in enumerate(answer_group['Question']):\n",
    "            for question_para_id, question_para in enumerate(get_para(question_orig)):\n",
    "                extended_FAQ['data'].append({\n",
    "                    'id': '{0}_{1}_{2}'.format(answer_id, question_id, question_para_id),\n",
    "                    'question': question_para,\n",
    "                    'answer': answer\n",
    "                })\n",
    "        answer_id += 1\n",
    "    with open(answers_filename, 'w') as fp:\n",
    "        json.dump(extended_answers, fp, indent=4)\n",
    "    with open(extended_filename, 'w') as fp:\n",
    "        json.dump(extended_FAQ, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c856da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob('*.tsv')\n",
    "file = file[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3546269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_tsv2.tsv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f787b6",
   "metadata": {},
   "source": [
    "## answer만 paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4995d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qg_test_answer_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKYE_RetrievalDB.csv\n",
      "already processed!\n",
      "NameLocGen.tsv\n",
      "already processed!\n",
      "qna_chitchat_caring.tsv\n",
      "already processed!\n",
      "qna_chitchat_enthusiastic.tsv\n",
      "already processed!\n",
      "qna_chitchat_friendly.tsv\n",
      "already processed!\n",
      "qna_chitchat_professional.tsv\n",
      "already processed!\n",
      "qna_chitchat_witty.tsv\n",
      "already processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob('*.csv') + glob.glob('*.tsv'):\n",
    "    print(file)\n",
    "    answers_filename = 'answers_' + file.replace(file[-4:], '.json')\n",
    "    if not overwrite and os.path.exists(answers_filename):\n",
    "        print('already processed!')\n",
    "        continue\n",
    "    extended_answers = {}\n",
    "    answer_groups = get_answer_groups(file)\n",
    "    answer_id = 0\n",
    "    for answer, answer_group in tqdm.tqdm(answer_groups):\n",
    "        extended_answers[answer] = get_para(answer)\n",
    "\n",
    "        answer_id += 1\n",
    "    with open(answers_filename, 'w') as fp:\n",
    "        json.dump(extended_answers, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be9cab5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5edde8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4c6c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b82ce",
   "metadata": {},
   "source": [
    "## question은 paraphrasing X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0648f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize paraphrasing model\n",
      "question_weather.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from textwrap import indent\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "print('initialize paraphrasing model')\n",
    "\n",
    "overwrite = False\n",
    "\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "    batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n",
    "\n",
    "def get_para(sentence):\n",
    "    para = get_response(sentence, num_return_sequences=5, num_beams=5)\n",
    "    if sentence not in para:\n",
    "        para.append(sentence)\n",
    "    return para\n",
    "\n",
    "def get_answer_groups(filename):\n",
    "    if filename.endswith('.tsv'):\n",
    "        db = pd.read_csv(filename, delimiter='\\t',encoding='utf-8-sig')\n",
    "    elif filename.endswith('.csv'):\n",
    "        db = pd.read_csv(filename,encoding='utf-8-sig')\n",
    "    answer_groups = db.groupby(db['answer'])\n",
    "    # print(len(answer_groups)) # 90 answers\n",
    "    return answer_groups\n",
    "\n",
    "for file in glob.glob('question_weather.csv'):\n",
    "    print(file)\n",
    "    answers_filename = 'answers_' + file.replace(file[-4:], '.json')\n",
    "    extended_filename = 'extended_' + file.replace(file[-4:], '.json')\n",
    "    if not overwrite and os.path.exists(answers_filename):\n",
    "        print('already processed!')\n",
    "        continue\n",
    "    extended_answers = {}\n",
    "    extended_FAQ = {'version': '1.0.0', 'data':[]}\n",
    "    answer_groups = get_answer_groups(file)\n",
    "    answer_id = 0\n",
    "    for answer, answer_group in tqdm.tqdm(answer_groups):\n",
    "        extended_answers[answer] = get_para(answer)\n",
    "        for question_orig, time_zone in zip(answer_group['question'],answer_group['time zone']):\n",
    "            for type in [\"How's\",\"How is\",\"What's\"]:\n",
    "                if time_zone == 'korner':\n",
    "                    extended_FAQ['data'].append({\n",
    "                    'question': type + ' the weather today?',\n",
    "                    'answer': answer,\n",
    "                    'time zone': time_zone,\n",
    "                    'region': question_orig\n",
    "                })\n",
    "                else:\n",
    "                    extended_FAQ['data'].append({\n",
    "                    'question': type + ' the weather in ' + question_orig + '?',\n",
    "                    'answer': type,\n",
    "                    'time zone': time_zone,\n",
    "                    'region': question_orig\n",
    "                })\n",
    "        answer_id += 1\n",
    "    with open(answers_filename, 'w',encoding='utf-8-sig') as fp:\n",
    "        json.dump(extended_answers, fp, indent=4,ensure_ascii=False)\n",
    "    with open(extended_filename, 'w',encoding='utf-8-sig') as fp:\n",
    "        json.dump(extended_FAQ, fp, indent=4,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8780d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
